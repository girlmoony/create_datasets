# create_datasets
Record how to create a datasets

### 总体思路
先定“类目→命名→准则”：  
把 300 类明确定义成可区分的视觉概念（如 salmon_nigiri vs tuna_nigiri，kappa_maki 等），写清楚：允许的角度、是否含配菜、是否允许带手/筷子、摆盘范围等。  

采集→去重→清洗→定稿：  
拿到原图后先做去重和质量过滤，再统一成 256×256 PNG（sRGB、无 alpha、三通道），最后再做 分层切分（避免同源图片跨集合泄漏）。  

轻增广、重一致性：  
训练时做轻量增广，验证/测试确保稳定、可复现。  

“小而极干净”的验证/测试：  
每类 30/30 尽量由双人复核，必要时溯源。  

### 目录与元数据  
```
datasets/
  train/
    XXXX/
      xxx_001.png
      ...
  val/
    ...
  test/
    ...
  metadata/
    classes.csv            # class_id,class_name,jp,alias
    dataset_card.md        # 收集、清洗、限制、许可
    split_seed.txt         # 例如：42
    sources.jsonl          # 每张图的来源/拍摄信息
```

### 关键标准与踩坑  
```
统一图像
色彩空间：全部转换为 sRGB；深度 8-bit；剔除 alpha；处理 EXIF 方向后再保存。
尺寸：先等比缩放到最长边≤256，再方形填充（建议反射填充）→ 256×256，再保存 PNG。

分割原则（140/30/30）
按来源分割（例如同一菜品拍摄的多连拍、同一家餐厅/同一视频帧 → 只能落在同一子集），避免近重复泄漏。
固定随机种子，输出 split 列表到 metadata/splits.json 便于复现。

可做的轻增广  
水平翻转（p=0.5）、±10°小旋转、轻微缩放裁剪、低强度色彩抖动、轻模糊/噪声。
不建议垂直翻转（食物重力/摆盘感觉会怪），不做过强颜色变换（否则三文鱼可能“变成”金枪鱼…）。

去重与质量
近重复：感知哈希（pHash/dHash），阈值如 Hamming ≤ 8 视为重复，留一张。
模糊/低质：Laplacian 方差阈值（例如 < 80 视为模糊，具体按你数据分布调）。

标注一致性
给每类写 3–5 张“边界样例”（应收/应拒），新图入库先过这套样例对比。

许可与合规
记录来源、许可；若包含人脸/店铺logo，按需打码或剔除。
```

### 自動データ整理  
pythonで作成  

### 数据量与磁盘粗估  
300 类 × 200 张 = 60,000 张。
256×256×3（8-bit）未压缩 ≈ 11.8 GB；PNG 压缩后常见 2–5 GB（取决于纹理/噪声）。

### 最终检查清单 
 ・类目清晰、示例边界明了（应收/应拒）。  
 ・所有图片 sRGB、三通道、256×256、无 EXIF 旋转残留。  
 ・近重复清理、模糊/遮挡过重剔除。  
 ・按来源分割，140/30/30，固定种子、可复现。  
 ・dataset_card.md 写明来源、许可、潜在偏差（如多为光照、缺少XX场景）。  
 ・训练/验证变换分离，验证集双人复核≥一次。  
